/*
 * Copyright 2024 Google LLC
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/cloud/aiplatform/v1beta1/vertex_rag_data.proto

// Protobuf Java Version: 3.25.5
package com.google.cloud.aiplatform.v1beta1;

public interface ImportRagFilesConfigOrBuilder
    extends
    // @@protoc_insertion_point(interface_extends:google.cloud.aiplatform.v1beta1.ImportRagFilesConfig)
    com.google.protobuf.MessageOrBuilder {

  /**
   *
   *
   * <pre>
   * Google Cloud Storage location. Supports importing individual files as
   * well as entire Google Cloud Storage directories. Sample formats:
   * - `gs://bucket_name/my_directory/object_name/my_file.txt`
   * - `gs://bucket_name/my_directory`
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.GcsSource gcs_source = 2;</code>
   *
   * @return Whether the gcsSource field is set.
   */
  boolean hasGcsSource();
  /**
   *
   *
   * <pre>
   * Google Cloud Storage location. Supports importing individual files as
   * well as entire Google Cloud Storage directories. Sample formats:
   * - `gs://bucket_name/my_directory/object_name/my_file.txt`
   * - `gs://bucket_name/my_directory`
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.GcsSource gcs_source = 2;</code>
   *
   * @return The gcsSource.
   */
  com.google.cloud.aiplatform.v1beta1.GcsSource getGcsSource();
  /**
   *
   *
   * <pre>
   * Google Cloud Storage location. Supports importing individual files as
   * well as entire Google Cloud Storage directories. Sample formats:
   * - `gs://bucket_name/my_directory/object_name/my_file.txt`
   * - `gs://bucket_name/my_directory`
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.GcsSource gcs_source = 2;</code>
   */
  com.google.cloud.aiplatform.v1beta1.GcsSourceOrBuilder getGcsSourceOrBuilder();

  /**
   *
   *
   * <pre>
   * Google Drive location. Supports importing individual files as
   * well as Google Drive folders.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.GoogleDriveSource google_drive_source = 3;</code>
   *
   * @return Whether the googleDriveSource field is set.
   */
  boolean hasGoogleDriveSource();
  /**
   *
   *
   * <pre>
   * Google Drive location. Supports importing individual files as
   * well as Google Drive folders.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.GoogleDriveSource google_drive_source = 3;</code>
   *
   * @return The googleDriveSource.
   */
  com.google.cloud.aiplatform.v1beta1.GoogleDriveSource getGoogleDriveSource();
  /**
   *
   *
   * <pre>
   * Google Drive location. Supports importing individual files as
   * well as Google Drive folders.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.GoogleDriveSource google_drive_source = 3;</code>
   */
  com.google.cloud.aiplatform.v1beta1.GoogleDriveSourceOrBuilder getGoogleDriveSourceOrBuilder();

  /**
   *
   *
   * <pre>
   * Slack channels with their corresponding access tokens.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.SlackSource slack_source = 6;</code>
   *
   * @return Whether the slackSource field is set.
   */
  boolean hasSlackSource();
  /**
   *
   *
   * <pre>
   * Slack channels with their corresponding access tokens.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.SlackSource slack_source = 6;</code>
   *
   * @return The slackSource.
   */
  com.google.cloud.aiplatform.v1beta1.SlackSource getSlackSource();
  /**
   *
   *
   * <pre>
   * Slack channels with their corresponding access tokens.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.SlackSource slack_source = 6;</code>
   */
  com.google.cloud.aiplatform.v1beta1.SlackSourceOrBuilder getSlackSourceOrBuilder();

  /**
   *
   *
   * <pre>
   * Jira queries with their corresponding authentication.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.JiraSource jira_source = 7;</code>
   *
   * @return Whether the jiraSource field is set.
   */
  boolean hasJiraSource();
  /**
   *
   *
   * <pre>
   * Jira queries with their corresponding authentication.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.JiraSource jira_source = 7;</code>
   *
   * @return The jiraSource.
   */
  com.google.cloud.aiplatform.v1beta1.JiraSource getJiraSource();
  /**
   *
   *
   * <pre>
   * Jira queries with their corresponding authentication.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.JiraSource jira_source = 7;</code>
   */
  com.google.cloud.aiplatform.v1beta1.JiraSourceOrBuilder getJiraSourceOrBuilder();

  /**
   *
   *
   * <pre>
   * SharePoint sources.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.SharePointSources share_point_sources = 13;</code>
   *
   * @return Whether the sharePointSources field is set.
   */
  boolean hasSharePointSources();
  /**
   *
   *
   * <pre>
   * SharePoint sources.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.SharePointSources share_point_sources = 13;</code>
   *
   * @return The sharePointSources.
   */
  com.google.cloud.aiplatform.v1beta1.SharePointSources getSharePointSources();
  /**
   *
   *
   * <pre>
   * SharePoint sources.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.SharePointSources share_point_sources = 13;</code>
   */
  com.google.cloud.aiplatform.v1beta1.SharePointSourcesOrBuilder getSharePointSourcesOrBuilder();

  /**
   *
   *
   * <pre>
   * The Cloud Storage path to write partial failures to.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.GcsDestination partial_failure_gcs_sink = 11;</code>
   *
   * @return Whether the partialFailureGcsSink field is set.
   */
  boolean hasPartialFailureGcsSink();
  /**
   *
   *
   * <pre>
   * The Cloud Storage path to write partial failures to.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.GcsDestination partial_failure_gcs_sink = 11;</code>
   *
   * @return The partialFailureGcsSink.
   */
  com.google.cloud.aiplatform.v1beta1.GcsDestination getPartialFailureGcsSink();
  /**
   *
   *
   * <pre>
   * The Cloud Storage path to write partial failures to.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.GcsDestination partial_failure_gcs_sink = 11;</code>
   */
  com.google.cloud.aiplatform.v1beta1.GcsDestinationOrBuilder getPartialFailureGcsSinkOrBuilder();

  /**
   *
   *
   * <pre>
   * The BigQuery destination to write partial failures to. It should be a
   * bigquery table resource name (e.g.
   * "bq://projectId.bqDatasetId.bqTableId"). If the dataset id does not
   * exist, it will be created. If the table does not exist, it will be
   * created with the expected schema. If the table exists, the schema will be
   * validated and data will be added to this existing table.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.BigQueryDestination partial_failure_bigquery_sink = 12;
   * </code>
   *
   * @return Whether the partialFailureBigquerySink field is set.
   */
  boolean hasPartialFailureBigquerySink();
  /**
   *
   *
   * <pre>
   * The BigQuery destination to write partial failures to. It should be a
   * bigquery table resource name (e.g.
   * "bq://projectId.bqDatasetId.bqTableId"). If the dataset id does not
   * exist, it will be created. If the table does not exist, it will be
   * created with the expected schema. If the table exists, the schema will be
   * validated and data will be added to this existing table.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.BigQueryDestination partial_failure_bigquery_sink = 12;
   * </code>
   *
   * @return The partialFailureBigquerySink.
   */
  com.google.cloud.aiplatform.v1beta1.BigQueryDestination getPartialFailureBigquerySink();
  /**
   *
   *
   * <pre>
   * The BigQuery destination to write partial failures to. It should be a
   * bigquery table resource name (e.g.
   * "bq://projectId.bqDatasetId.bqTableId"). If the dataset id does not
   * exist, it will be created. If the table does not exist, it will be
   * created with the expected schema. If the table exists, the schema will be
   * validated and data will be added to this existing table.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.BigQueryDestination partial_failure_bigquery_sink = 12;
   * </code>
   */
  com.google.cloud.aiplatform.v1beta1.BigQueryDestinationOrBuilder
      getPartialFailureBigquerySinkOrBuilder();

  /**
   *
   *
   * <pre>
   * Specifies the size and overlap of chunks after importing RagFiles.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.RagFileChunkingConfig rag_file_chunking_config = 4;
   * </code>
   *
   * @return Whether the ragFileChunkingConfig field is set.
   */
  boolean hasRagFileChunkingConfig();
  /**
   *
   *
   * <pre>
   * Specifies the size and overlap of chunks after importing RagFiles.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.RagFileChunkingConfig rag_file_chunking_config = 4;
   * </code>
   *
   * @return The ragFileChunkingConfig.
   */
  com.google.cloud.aiplatform.v1beta1.RagFileChunkingConfig getRagFileChunkingConfig();
  /**
   *
   *
   * <pre>
   * Specifies the size and overlap of chunks after importing RagFiles.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.RagFileChunkingConfig rag_file_chunking_config = 4;
   * </code>
   */
  com.google.cloud.aiplatform.v1beta1.RagFileChunkingConfigOrBuilder
      getRagFileChunkingConfigOrBuilder();

  /**
   *
   *
   * <pre>
   * Specifies the parsing config for RagFiles.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.RagFileParsingConfig rag_file_parsing_config = 8;</code>
   *
   * @return Whether the ragFileParsingConfig field is set.
   */
  boolean hasRagFileParsingConfig();
  /**
   *
   *
   * <pre>
   * Specifies the parsing config for RagFiles.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.RagFileParsingConfig rag_file_parsing_config = 8;</code>
   *
   * @return The ragFileParsingConfig.
   */
  com.google.cloud.aiplatform.v1beta1.RagFileParsingConfig getRagFileParsingConfig();
  /**
   *
   *
   * <pre>
   * Specifies the parsing config for RagFiles.
   * </pre>
   *
   * <code>.google.cloud.aiplatform.v1beta1.RagFileParsingConfig rag_file_parsing_config = 8;</code>
   */
  com.google.cloud.aiplatform.v1beta1.RagFileParsingConfigOrBuilder
      getRagFileParsingConfigOrBuilder();

  /**
   *
   *
   * <pre>
   * Optional. The max number of queries per minute that this job is allowed to
   * make to the embedding model specified on the corpus. This value is specific
   * to this job and not shared across other import jobs. Consult the Quotas
   * page on the project to set an appropriate value here.
   * If unspecified, a default value of 1,000 QPM would be used.
   * </pre>
   *
   * <code>int32 max_embedding_requests_per_min = 5 [(.google.api.field_behavior) = OPTIONAL];
   * </code>
   *
   * @return The maxEmbeddingRequestsPerMin.
   */
  int getMaxEmbeddingRequestsPerMin();

  com.google.cloud.aiplatform.v1beta1.ImportRagFilesConfig.ImportSourceCase getImportSourceCase();

  com.google.cloud.aiplatform.v1beta1.ImportRagFilesConfig.PartialFailureSinkCase
      getPartialFailureSinkCase();
}
